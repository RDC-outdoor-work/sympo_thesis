\documentclass[uplatex, twocolumn, 9pt]{jsproceedings}
\RequirePackage[l2tabu, orthodox]{nag}  % 古いコマンドやパッケージを使用した場合に警告する
\usepackage[all, warning]{onlyamsmath}  % amsmath が提供しない数式環境を使用した場合に警告する
\usepackage{flushend}  % 最終ページの2カラムの左右の高さを揃える
\usepackage{caption}
\usepackage{graphicx}

% タイトル
\title{つくばチャレンジ2022における\\千葉工業大学未来ロボティクス学科チームの取り組み}

\author{○筑波 太郎\authorrefmark{1}，筑波 花子\authorrefmark{1}}

\etitle{Development Activity of Advanced Robotics Department Team of Chiba Institute of Technology at Tsukuba Challenge 2021}

\eauthor{*Taro TSUKUBA\eauthorrefmark{1}, Hanako TSUKUBA\eauthorrefmark{1}}

\affiliation{千葉工業大学未来ロボティクス学科チームα, box, box2}

\abstract{
  In this paper, we introduce the activities of the Future Robotics Department, Advanced Engineering Division, Chiba Institute of Technology team in the Tsukuba Challenge 2022. Our team is working on the development of an outdoor autonomous mobile robot, and we are currently tackling several challenges. For example, the development of a new robot, driving using semantic segmentation, and recovery of kidnapped state by combining expansion resetting and GNSS resetting. We will report the results of these efforts.
}

% \keywords{\LaTeXe, Class File, Proceedings}

\begin{document}
\maketitle

% \authorreftext{1}{筑波大学 大学院 システム情報工学研究科}
% \eauthorreftext{1}{Graduate School of Systems and Information Engineering,\\ University of Tsukuba}

% 本文
\section{はじめに}
本チームは, 屋外で安定して自律移動するロボットを目指し, その研究および開発の一環としてつくばチャレンジに参加している. 開発したシステムは2次元地図と測域センサを用いた自己位置推定により, つくばチャレンジ2016, 2017においてマイルストーン3「横断歩道区間を含まない課題コース(2037m)」を達成した. しかし, つくばチャレンジ2018から, ひらけた公園がコースに含まれるようになった. そのため, 30m程度の2次元レーザレンジセンサでは検出可能な物体が比較的少ないことから, 従来のシステムでは安定して自己位置推定を行うことが困難になった. 2018年度から計測距離が100mの3次元レーザレンジセンサを採用したが, 水平面の計測データのみを使用しており, 多くのデータを破棄していた. さらに, 公園内では場所により地面の傾斜が変化するため, レーザが必ずしも水平の距離を計測していないという問題もあった. 例えば, 斜めに土が積み上げられた場所では, レーザを照射する位置が上下に変化すると, それに合わせて距離も変化してしまう. そのため, 地図生成が容易に行えないという問題もあった. 2019年度は, これらの3次元データを有効に活用するため, 2次元地図ではなく, 3次元地図を生成しMCLによる3次元自己位置推定を行い, 2次元自己位置推定と比べ, 自己位置が安定していることを確認した. しかし, 公園のような開けた場所では測域センサで検出できる物体が少なく, 測域センサ, オドメトリを用いる方法では自己位置が定まらないことがある. 本稿では, このような課題を解決するためにつくばチャレンジ2022に向けて取り組んだ開発に関して紹介する. 

\section{ロボットの概要}
つくばチャレンジでは, 本チームが開発を続けている三台のロボットORNE-α, ORNE-box, ORNE-box2を用いる. それぞれの方針は以下の通りである. 
\begin{itemize}
  \item ORNE-α\\2つの走行の切り替えによる安定した自律走行
  \item ORNE-box, ORNE-box2\\ 
\end{itemize}

\subsection{ハードウェア}
本チームは屋外自律移動ロボットとして, ORNE-α, ORNE-box, ORNE-box2の開発を行っており, つくばチャレンジ2022にはこれらのロボットが参加する. \figref{fig:orne-series}に本チームの開発している自律移動ロボットの外観を示す. これらのロボットはi-Cart middleをベースとしており, 主なセンサはIMU, 測域センサである. 

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[width=30mm]{fig/alpha.pdf}
    \caption*{(a) ORNE-α}
  \end{minipage} 
  \hspace{0.03\columnwidth}
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[height=34mm]{fig/box.pdf}
    \caption*{(b) ORNE-box}
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[height=34mm]{fig/box2.pdf}
    \caption*{(c) ORNE-box2}
  \end{minipage}
  \caption{ORNE-Series}
  \label{fig:orne-series}%\vspace*{-2mm}
\end{figure}

\begin{table}[h]
  \centering
    \caption{Specifications of the robots}
    \scalebox{0.8}{
    \begin{tabular}{|l||c|c|c|}  \hline
       & ORNE-α & ORNE-box & ORNE-box2 \\ \hline \hline
      Depth[mm] & 690 & \multicolumn{2}{|c|}{600} \\ \hline
      Wide[mm] & 560 & \multicolumn{2}{|c|}{506.5}\\ \hline
      Height[mm] & 770 & \multicolumn{2}{|c|}{957}\\ \hline
      Wheel diameter[mm] & \multicolumn{3}{|c|}{304}\\ \hline
      Battery & \multicolumn{3}{|c|}{LONG WP12-12}\\ \hline
      Motor & \multicolumn{3}{|c|}{Oriental motor TF-M30-24-3500-G15L/R}\\ \hline
      Driving system & \multicolumn{3}{|c|}{Power wheeled steering}\\ \hline
      2D-LiDAR & URM-40LC-EW & None & UTM-30LX-EW\\
      & (HOKUYO) & & (HOKUYO)\\ \hline
      3D-LiDAR & None & R-fans-16 & VLP-16\\
      & & (SureStar) & (Velodyne)\\ \hline
      IMU & ADIS16465 & \multicolumn{2}{|c|}{ADIS16475}\\ 
      & (Analog devices) & \multicolumn{2}{|c|}{Analog devices}\\ \hline
      GNSS receiver & None & \multicolumn{2}{|c|}{u-blox SCR-u2t}\\ \hline
      Camera & CMS-V43BK & \multicolumn{2}{|c|}{None}\\ 
      & (Sanwa supply) & \multicolumn{2}{|c|}{}\\ \hline
    \end{tabular}
  }
  \end{table}

  \newpage

\subsection{ソフトウェア}
本チームでは, 従来よりROS(Robot Operating System)のnavigation stack\cite{navigation}をもとに開発されたシステムであるorne\_navigationにより自律走行させている. \figref{fig:soft-fig}に開発しているロボットのソフトウェアを含むシステム構成を示す. このシステムは, 2D-LiDARを用いたMonte Carlo Localization(MCL)により確率的に自己位置を推定し, 経路計画に基づいて自律走行している. また, GitHubのopen-rdc\cite{rdc}でプロクラムを公開している.


\begin{figure}[h]
  \centering
  \includegraphics[width=85mm]{fig/software.pdf}
  \caption{Structure of the system.}
  \label{fig:soft-fig}%\vspace*{-2mm}
\end{figure}

\section{各チームの方針}
本チームには, チームORNE-α, ORNE-box, ORNE-box2の3チームが存在する. 従って本章では, 各チームごとの活動方針を述べる. ただし, ORNE-box2はORNE-boxの後継機であるため, 活動方針は同じである.

\subsection{チームORNE-α}
2D-LiDARベースの自律走行時, 自己位置推定の結果が不確かになる場合がある. この状態での走行はリタイアの要因の一つになる可能性がある. そこで, チームORNE-αは, 2D-LiDARベースの自律走行と機械学習を用いた自律走行の切り替えによる安定した走行を目的としている. 昨年度は, orne\_navigationによる自律走行時, 自己位置推定の尤度が低下した場合に, カメラ画像を入力としたend-to-end学習器を用いた自律走行による切り替えを行った. しかし, 意図しない箇所でカメラを用いた走行へ切り替えが起こってしまうことがあった. そのため, 本年度はそれらの問題を解決するために, 取り組んだ内容に関して以下で紹介する. 

\subsubsection{提案手法}
提案手法を\figref{fig:kirikae}に示す. 移動ロボットは, 2つの走行方法を持つ. この2つの走行方法は, emcl2[3]のalpha\footnote[1]{センサ更新後のパーティクルの周辺尤度}を指標として切り替える.

\begin{figure}[h]
  \centering
  \includegraphics[width=85mm]{fig/kirikae.pdf}
  \caption{Developed system of switching action.}
  \label{fig:kirikae}%\vspace*{-2mm}
\end{figure}

\newpage

\begin{itemize}
  \item alphaが高い場合\\
  ①LiDARベースのナビゲーション
  \item alphaが低い場合\\
  ②セマンティックセグメンテーションを用いた走行
\end{itemize}

\subsubsection{2D-LiDARベースの自律走行時について}

\begin{itemize}
  \item 外界センサ: 2DLiDAR
  \item 自己位置推定: emcl2
  \item global planner: A*
  \item local planner: dwa\_local\_planner
  \item 地図作成: cartographer
  \begin{itemize}
    \item resolution: 0.15[m/pixel]
    \item 確認走行, 駅周辺, 公園の3つに分割したものを合成
  \end{itemize}
\end{itemize}

\subsubsection{セマンティックセグメンテーションについて}
フレームワーク: DeeplabV3 mobilenet
\subsubsection{ロボットの制御}
セマンティックセグメンテーションを用いて, 領域分割を行う. 処理を施した例を\figref{fig:for_seg}に示す.

\begin{figure}[h]
  \centering
  \includegraphics[width=60mm]{fig/camera_for_seg.pdf}
  \caption{The top image is the original image, the bottom image is the processed image using semantic segmentation}
  \label{fig:for_seg}%\vspace*{-2mm}
\end{figure}

これらの処理を施した画像を用いて, \figref{fig:seg}の対応する箇所に応じた行動を生成する. 具体的には, \figref{fig:seg}のTurn leftの範囲に走行可能領域以外のラベルによるピクセル数がある程度多くなると, 左に曲がるヨー方向の角速度を出力する. また, GitHubのdeeplabv3\_plus\_pytorch\_ros\cite{DeeplabV3}でプログラムとデータセットを公開している.

\begin{figure}[h]
  \centering
  \includegraphics[width=70mm]{fig/seg.pdf}
  \caption{Types of behavior using semantic segmentation}
  \label{fig:seg}%\vspace*{-2mm}
\end{figure}

\newpage

\subsubsection{チームORNE-αの本走行の結果と展望}
今年度の本走行の記録は847.9[m]で, 駅構内の手前でリタイアとなった. これは歩行者などによりランドマークが隠され, 自己位置が誘拐されるなどの複合的な要因によるものだと考えられる. 今後は, 自己位置推定に用いるセンサを追加するなどの対策を検討する. 

\subsection{チームORNE-box, ORNE-box2}


\subsection{本文中での参照}
論文に挿入した図は，本文中で必ず参照する．
図を参照する際は，以下の例のように \verb*|\figref{}| コマンドを用いる．
\verb*|\figref{}| コマンドは，jsproceedings.cls で定義した独自のコマンドである．
参照先の図には，\verb*|\label{}| コマンドでラベルを付けておく．
\begin{description}[style=nextline]
  \item[\LaTeX ソース]%\leavevmode \\
  \verb|\figref{fig:matching-concept}に，提案手法の概念と各変数の定義を示す．|
  \item[出力]%\leavevmode \\
  % \figref{fig:matching-concept}に，提案手法の概念と各変数の定義を示す．
\end{description}


\section{表}
本章では，表を挿入する方法，キャプションの書き方や本文中での参照の仕方について述べる．

\subsection{Table の挿入}
% \tableref{table:corr-dist}に，表を挿入する例を示す．
表の挿入には，table 環境（\verb*|\begin{table}|，\verb*|\end{table}|）と tabular 環境（\verb*|\begin{tabular}|，\verb*|\end{tabular}|）を用いる．
具体的な書き方は \LaTeX ソースを参照されたい．

% \begin{table}
%   \centering
%   \caption{Means of the correspondence distances after registration.}
%   \label{table:corr-dist}
%   \begin{tabular}{cccc}
%   \toprule%[0.08em]  % デフォルト 0.08em（\heavyrulewidth）
%   \textbf{Environment} & room & hallway & moved objects\\
%   \midrule%[0.05em]  % デフォルト 0.05em（\lightrulewidth）
%   \textbf{Standard ICP} & 0.123 [m] & 0.456 [m] & 0.789 [m]\\
%   \textbf{MAP-ICP} & 0.987 [m] & 0.654 [m] & 0.321 [m]\\
%   \bottomrule%[0.08em]  % デフォルト 0.08em（\heavyrulewidth）
%   \end{tabular}
% % \vspace*{-2mm}
% \end{table}

表の罫線は格子状に引く必要はなく，省略できる罫線は引かずに罫線を少なくした表の方が美しい組版と言われる．
特に英文の場合，縦罫線は引かない方が良い．

横罫線は，標準の \verb*|\hline| コマンドでは上下の間隔が狭く，線の太さのバランスも良くない．
% そこで booktabs.sty の \verb*|\toprule|，\verb*|\midrule|，\verb*|\cmidrule|，\verb*|\bottomrule| コマンドを用いることで，より見やすい表を書く．

\subsection{キャプション}
表の上に \verb*|\caption{}| コマンドでキャプションを付ける．
キャプションを英語で書くか，日本語で書くかを論文中で統一する．
論文を投稿する学会のフォーマットに従うが，和文論文でも英語のキャプションとする場合が多い．
図表とキャプションだけを見て論文の内容が類推できるよう，キャプションは単語ではなく文章で書く．
よって英語の場合は最初の文字を大文字にし，その後は固有名詞などを除いて小文字にする．
また，文末にはピリオドを書く．

\subsection{本文中での参照}
論文に挿入した表は，本文中で必ず参照する．
表を参照する際は，以下の例のように \verb*|\tableref{}| コマンドを用いる．
\verb*|\tableref{}| コマンドは，jsproceedings.cls で定義した独自のコマンドである．
参照先の表には，\verb*|\label{}| コマンドでラベルを付けておく．
\begin{description}[style=nextline]
  \item[\LaTeX ソース]%\leavevmode \\
  \verb|\tableref{table:corr-dist}に，マッチング後の対応点間距離の平均値を示す．|
  \item[出力]%\leavevmode \\
  % \tableref{table:corr-dist}に，マッチング後の対応点間距離の平均値を示す．
\end{description}


\section{おわりに}
本稿では,千葉工業大学未来ロボティクス学科チームで開
発しているロボットの概要とシステムの構成に関して述べた.
また,つくばチャレンジ 2022 に向けた取り組みについて紹介
した.

% 参考文献
% \small
\footnotesize
\begin{thebibliography}{99}

\bibitem{navigation}
ros-planning, navigation レポジトリ\\
\url{https://github.com/ros-planning/navigation}
(最終閲覧日:2022年12月5日)

\bibitem{rdc}
Robot Design and Control Lab, openrdc orne\_navigation レポジトリ\\
\url{https://github.com/open-rdc/orne_navigation}
(最終閲覧日:2022年12月5日)

\bibitem{DeeplabV3}
deeplabv3\_plus\_pytorch\_ros レポジトリ\\
\url{https://github.com/Tsumoridesu/deeplabv3_plus_pytorch_ros/tree/add_cmd_vel}
(最終閲覧日:2022年12月5日)

\end{thebibliography}
\normalsize

\end{document}
