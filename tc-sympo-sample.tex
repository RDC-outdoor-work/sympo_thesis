\documentclass[uplatex, twocolumn, 9pt]{jsproceedings}
\RequirePackage[l2tabu, orthodox]{nag}  % 古いコマンドやパッケージを使用した場合に警告する
\usepackage[all, warning]{onlyamsmath}  % amsmath が提供しない数式環境を使用した場合に警告する
\usepackage{flushend}  % 最終ページの2カラムの左右の高さを揃える
\usepackage{caption}
\usepackage{graphicx}
\usepackage{amssymb, amsmath}

% タイトル
\title{つくばチャレンジ2022における\\千葉工業大学未来ロボティクス学科ORNEチームの取り組み}

\author{○藤原 柾, 清岡 優祐, 大塚 竜征, 石 涛, 春山 健太, 井口 颯人, 髙橋 祐樹, 白須 和暉, 野村 駿斗, 桜井 真希\newline 上田 隆一, 林原 靖男 （千葉工大）}

\etitle{Development Activity of Advanced Robotics Department Team of Chiba Institute of Technology at Tsukuba Challenge 2022}

\eauthor{Masaki FUJIWARA, Yusuke KIYOOKA, Ryusei OTSUKA, Tou SEKI, Kenta HARUYAMA, Hayato IGUCHI, Yuki TAKAHASHI, Kazuki SHIRASU, Hayato NOMURA, Maki SAKURAI, \newline Ryuichi UEDA and Yasuo HAYASHIBARA (CIT)}

\affiliation{千葉工業大学未来ロボティクス学科}

\abstract{
  In this paper, we introduce the activities of the Future Robotics Department, Advanced Engineering Division, Chiba Institute of Technology team in the Tsukuba Challenge 2022. Our team is working on the development of an outdoor autonomous mobile robot, and we are currently tackling several challenges. For example, the development of a new robot, driving using semantic segmentation, and recovery of kidnapped state by combining expansion resetting. We will report the results of these efforts.
}

% \keywords{\LaTeXe, Class File, Proceedings}

\begin{document}
\maketitle

% \authorreftext{1}{筑波大学 大学院 システム情報工学研究科}
% \eauthorreftext{1}{Graduate School of Systems and Information Engineering,\\ University of Tsukuba}

% 本文
\section{はじめに}
% 本チームは, 屋外で安定して自律移動するロボットを目指し, その研究および開発の一環としてつくばチャレンジに参加している. 開発したシステムは2次元地図と2D-LiDARを用いた自己位置推定により, つくばチャレンジ2016, 2017においてマイルストーン3「横断歩道区間を含まない課題コース(2037m)」を達成した. しかし, つくばチャレンジ2018から, ひらけた公園がコースに含まれるようになった. そのため, 計測距離が30m程度の2D-LiDARでは検出可能な物体が比較的少ないことから, 従来のシステムでは安定して自己位置推定を行うことが困難になった. 2018年度から計測距離が100mの3D-LiDARを採用したが, 水平面の計測データのみを使用しており, 多くのデータを破棄していた. さらに, 公園内では場所により地面の傾斜が変化するため, レーザが必ずしも水平の距離を計測していないという問題もあった. 例えば, 斜めに土が積み上げられた場所では, レーザを照射する位置が上下に変化すると, それに合わせて距離も変化してしまう. そのため, 地図生成が容易に行えないという問題もあった. 2019年度は, これらの3次元データを有効に活用するため, 2次元地図ではなく, 3次元地図を生成しMCLによる3次元自己位置推定を行い, 2次元自己位置推定と比べ, 自己位置が安定していることを確認した. しかし, 公園のような開けた場所ではLiDARで検出できる物体が少なく, LiDAR, オドメトリを用いる方法では自己位置が定まらないことがある. 本稿では, このような課題を解決するためにつくばチャレンジ2022に向けて取り組んだ開発に関して紹介する. 

% 本稿では, 千葉工業大学未来ロボティクス学科ORNEチームの取り組みについて紹介する. 
本チームは, 屋外で自律移動するロボットの研究開発を行っており, その一環としてつくばチャレンジに参加している. 本チームでは公園における地図生成や自己位置推定を中心に研究開発を行ってきた. また, 防水機能や高い拡張性を有したオープンプラットフォームロボットの開発も行っている. 

% つくばチャレンジ2021では,  
本稿では, 
% このような課題を解決するために
つくばチャレンジ2022に向けて取り組んだ内容に関して紹介する.

\section{ロボットの概要}
本チームには, 開発を続けている三台のロボットORNE-α, ORNE-box\cite{si-box}, ORNE-box2がある. 各ロボットごとの開発方針は以下の通りである. 
\begin{itemize}
  \item ORNE-α\\LiDARとカメラを用いた自律走行
  \item ORNE-box, ORNE-box2\\3次元自己位置推定による安定した自律走行
\end{itemize}

\subsection{ハードウェア}
次に, 各ロボットの外観を\figref{fig:orne-series}, ハードウェア構成を\tableref{tb:hardware}に示す. これらのロボットはi-Cart mini, i-Cart middleをベースにしている.

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[width=30mm]{fig/alpha.pdf}
    \caption*{(a) ORNE-α}
  \end{minipage} 
  \hspace{0.03\columnwidth}
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[height=34mm]{fig/boxkai.pdf}
    \caption*{(b) ORNE-box}
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[height=34mm]{fig/box2kai.pdf}
    \caption*{(c) ORNE-box2}
  \end{minipage}
  \caption{ORNE-Series}
  \label{fig:orne-series}%\vspace*{-2mm}
\end{figure}

\begin{table}[h]
  \centering
    \caption{Specifications of the robots}
    \scalebox{0.8}{
    \begin{tabular}{|l||c|c|c|}  \hline
       & ORNE-α & ORNE-box & ORNE-box2 \\ \hline \hline
      Depth[mm] & 690 & \multicolumn{2}{|c|}{600} \\ \hline
      Wide[mm] & 560 & \multicolumn{2}{|c|}{506.5}\\ \hline
      Height[mm] & 770 & \multicolumn{2}{|c|}{957}\\ \hline
      Wheel diameter[mm] & \multicolumn{3}{|c|}{304}\\ \hline
      Battery & LONG WP12-12 & \multicolumn{2}{|c|}{LONG WP14-12SE}\\ \hline
      Motor & \multicolumn{3}{|c|}{Oriental motor TF-M30-24-3500-G15L/R}\\ \hline
      Driving system & \multicolumn{3}{|c|}{Power wheeled steering}\\ \hline
      2D-LiDAR & URM-40LC-EW & None & UTM-30LX-EW\\
      & (HOKUYO) & & (HOKUYO)\\ \hline
      3D-LiDAR & None & R-Fans-16 & VLP-16\\
      & & (SureStar) & (Velodyne)\\ \hline
      IMU & \multicolumn{2}{|c|}{ADIS16465} & ADIS16470\\ 
      & \multicolumn{2}{|c|}{(Analog devices)} & Analog devices\\ \hline
      % GNSS receiver & None & \multicolumn{2}{|c|}{zed-f9p}\\ \hline
      Camera & CMS-V43BK & \multicolumn{2}{|c|}{None}\\ 
      & (Sanwa supply) & \multicolumn{2}{|c|}{}\\ \hline
    \end{tabular}
  }
  \label{tb:hardware}
  \end{table}

  \newpage

\subsection{ソフトウェア}
従来よりROS(Robot Operating System)のnavigation stack\cite{navigation}をもとに開発されたシステムであるorne\_navigationとorne-box\cite{box}によりロボットを自律走行させている. \figref{fig:soft-fig}に開発しているロボットのソフトウェアを含むシステム構成を示す. また, 両パッケージ共にGitHubのopen-rdc\cite{rdc}でプロクラムを公開している. 

\begin{figure}[h]
  \centering
  \includegraphics[width=85mm]{fig/software.pdf}
  \caption{Structure of the system}
  \label{fig:soft-fig}%\vspace*{-2mm}
\end{figure}

\newpage
\section{各ロボットごとの研究開発}
本チームは, つくばチャレンジにおいて各ロボットごとに研究開発の目的が異なる. 本章では, 各ロボットごとの取り組みを述べる.

\subsection{ORNE-α}
LiDARを用いた自律走行時, 自己位置推定の結果が不確かになる場合がある. この状態での走行はリタイアの要因の一つになる. そこで, ORNE-αは, 2D-LiDARを用いた自律走行と機械学習を用いた自律走行の切り替えにより, つくばチャレンジをコースアウトせずに完走することを目的としている. \par 2021年度は, orne\_navigationによる自律走行時, 自己位置推定の尤度が低下した場合に, カメラ画像を入力としたend-to-end学習器\footnote[1]{入力から出力までを一括して学習するようなニューラルネットワーク}\cite{end-to-end}の出力を用いた自律走行による切り替えを行った. しかし, 意図しない箇所でカメラを用いた走行へ切り替えが起こってしまった. そのため, 本年度はこの問題を解決するために, 取り組んだ内容に関して以下で紹介する. 

\subsubsection{周辺尤度を用いた2つの走行方法の切り替え}
ORNE-αは, 2つの走行方法を持つ. 切り替え方法を\figref{fig:switching}に示す. この2つの走行方法は, 自己位置推定パッケージであるemcl2\cite{emcl2-git}の周辺尤度:alpha\footnote[2]{センサ更新後のパーティクルの周辺尤度(0 ≦ α ≦ 1)}を指標として切り替える. ただし, 本走行では②の自律走行に切り替わるのは折り返し地点からである. 

\begin{figure}[h]
  \centering
  \includegraphics[width=90mm]{fig/switching3.pdf}
  \caption{Developed system of switching action}
  \label{fig:switching}%\vspace*{-2mm}
\end{figure}

% \newpage
\begin{itemize}
  \item [①]alphaが0でない場合\par
  2D-LiDARを用いた自律走行を行う. システム構成は以下に示す. 
  \begin{itemize}
    \item 外界センサ: 2D-LiDAR
    \item 自己位置推定パッケージ: emcl2
    \item global planner: A*
    \item local planner: dwa\_local\_planner
    \item 地図作成: Google Cartographer
    \begin{itemize}
      \item resolution: 0.15[m/pixel]
      \item 確認走行エリア, 駅周辺, 公園エリアのそれぞれで作成した地図を合成
    \end{itemize}
  \end{itemize}
  \item [②]alpha = 0が3秒続いた場合\\
  カメラ画像を入力とした深層学習の出力で自律走行し, セマンティックセグメンテーション\cite{Deeplab}により走行可能領域を抽出, 行動生成を行う. \\
\end{itemize}

  \newpage
  \begin{enumerate}
    \item 走行可能領域の検出\\
  % カメラ画像は前述のモデルを通して, \figref{fig:seg_runarea}の左画像を獲得する. 
  走行可能領域を抽出するために画像を2値化する. その後, モルフォロジー変換\cite{morphological}でノイズを除去する. 結果は\figref{fig:seg_runarea}の右画像に示す.
  \begin{figure}[h]
    \centering
    \includegraphics[width=100mm]{fig/tsukuba-seg.pdf}
    \caption{The left image uses semantic segmentation, and the right image shows the extracted driving area as a result}
    \label{fig:seg_runarea}%\vspace*{-2mm}
  \end{figure}
  % \newpage
  \item 行動生成\\
  1)の処理を施した画像を用いて, \figref{fig:seg}の6つの対応する領域に応じた行動を生成する. この行動生成に関しては先行研究\cite{meiji-thesis}を参考に実装した. 例えば, \figref{fig:seg}のTurn leftの範囲に走行可能領域以外が多く存在する場合, 左に曲がる方向の角速度を出力する. 
  \begin{figure}[h]
    \centering
    \includegraphics[width=60mm]{fig/seg.pdf}
    \caption{Types of behavior using semantic segmentation}
    \label{fig:seg}%\vspace*{-2mm}
  \end{figure}
\end{enumerate}

\subsubsection{実験}
次に, 2つの走行方法を切り替える手法の有効性を検証するため, 千葉工業大学 津田沼キャンパスにおいて\figref{fig:tsudanuma}に示す経路で実験を行う. 学習に使用したデータセットは, Cityscapes Dataset\cite{cityscapes}(5000枚), 千葉工業大学 津田沼キャンパス(66枚), モデルはDeepLabV3Plus-MobileNetで学習を行った. また, 使用したPCのスペックを\tableref{table:pc}に示す. 

\begin{figure}[h]
  \centering
  \includegraphics[width=70mm]{fig/tsudanuma-seg.pdf}
  \caption{Map of Chiba Institute of Technology Tsudanuma Campus from \cite{tsudanuma}}
  \label{fig:tsudanuma}%\vspace*{-2mm}
\end{figure}

\begin{table}[h]
  \centering
  \caption{Specification of PC}
  \label{table:pc}
  \begin{tabular}{cc}
  \toprule%[0.08em]  % デフォルト 0.08em（\heavyrulewidth）
  \textbf{CPU} & Core i7-9750H(Intel)\\
  % \midrule%[0.05em]  % デフォルト 0.05em（\lightrulewidth）
  \textbf{RAM} & 16GB\\
  \textbf{GPU} & RTX 2070 Max-Q\\
  \bottomrule%[0.08em]  % デフォルト 0.08em（\heavyrulewidth）
  \end{tabular}
% \vspace*{-2mm}
\end{table}

\newpage
実験の様子を\figref{fig:switch-mode}に示す. (a):パーティクル拡散前は2D-LiDARを用いて走行し, (b):パーティクル拡散時, カメラ画像を入力とした深層学習の出力で走行している. その後, (c):パーティクルの収束後では, 再び2D-LiDARを用いた走行に切り替わっている. これらから, 2つの走行方法の切り替えにより, \cite{tsudanuma}で示した経路をコースアウトすることなく走行することができた. 

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{1\linewidth}
    \centering
    \includegraphics[width=70mm]{fig/switch-mode1.pdf}
    \caption*{(a) Before particle diffusion}
  \end{minipage} 
  % \newpage
  \hspace{0.03\columnwidth}
  \begin{minipage}[b]{1\linewidth}
    \centering
    \includegraphics[width=70mm]{fig/switch-mode2.pdf}
    \caption*{(b) Diffusion of particles}
  \end{minipage}
  \begin{minipage}[b]{1\linewidth}
    \centering
    \includegraphics[width=70mm]{fig/switch-mode3.pdf}
    \caption*{(c) After particle convergence}
  \end{minipage}
  \caption{Experiments on the Tsudanuma campus grounds}
  \label{fig:switch-mode}%\vspace*{-2mm}
\end{figure}

% つくば公園エリア周辺画像(222枚)
% また, GitHubのdeeplabv3\_plus\_pytorch\_ros\cite{DeeplabV3}でプログラムとデータセットを公開している.

\newpage
\subsubsection{本走行の結果}
2022年度の本走行の記録は847.9[m]で, 駅構内の手前でリタイアとなった. これは歩行者などによりランドマークが隠されたことで, \figref{fig:kidnapped}に示すように, 自己位置推定の結果が不確かになり, 真の姿勢の周囲にパーティクルが存在しない状態である誘拐状態\cite{emcl-thesis}になったことが要因の一つだと考えられる. また, 折り返し地点まで辿り着けなかったため, カメラ画像を用いた自律走行への切り替えによるコースアウトを防ぐ挙動が確認できなかった. しかし, 実験走行は折り返し地点周辺で

\begin{figure}[h]
  \centering
  \includegraphics[width=80mm]{fig/yuukai2.pdf}
  \caption{The result of self-location estimation became uncertain, resulting in a state of kidnapping.}
  \label{fig:kidnapped}%\vspace*{-2mm}
\end{figure}

% \newpage
\subsection{ORNE-box, ORNE-box2}
屋外で自律移動させるためには防水が必要となるが, 防水性を担保しながらロボットの機構や電気回路の追加・変更を行うことは多くの時間を要する. これにより, 本来研究として実施する新しいアルゴリズムの開発や検証に十分な時間を割り当てられず問題となっていた. このような問題に対して, 屋外自律移動ロボットプラットフォームORNE-boxの開発\cite{si-box}を行ってきた．このロボットは屋外での自律走行を目的としており，屋外自律走行の研究開発に必要な機能をパッケージ化し提供することを目標に開発を行っている．現在はウェブページ上\cite{box}にて設計データなどを順次公開している. \par 
ORNE-box2においては, ORNE-boxにおける問題を洗い出し改善を図るという方針のもと, 今年度から開発を行っている．センサ構成などに差異はあるが，基本的なシステム構成は同じである．

\subsubsection{3D-LiDARを用いた自律走行について}
つくばチャレンジにおいてORNE-boxとORNE-box2は, 3D-LiDARを用いた自己位置推定を行っている. 自己位置推定にはROSのパッケージであるmcl-3dl\cite{mcl-3dl}を用いた. mcl-3dlは3D-LiDAR及びIMU, オドメトリを入力としてMCLを行っている.

\subsubsection{本走行の結果}
ORNE-boxの今年度の本走行の記録は1000[m]だった. リタイアした地点は公園前横断歩道であり, 縁石の縁端を検出できず乗り上げてスタックした. 本年度のORNE-boxは\tableref{tb:hardware}からもわかるように障害物検出は3D-LiDARのみで行っていた. 3D-LiDARは機体上部に位置しており, スタックの原因となった縁石の縁端のような低層障害物の検出は困難であった．今後は低層障害物も検出できるよう，障害物検出の改善を図る．\par
ORNE-box2の本走行の記録は, 一時停止位置で停止できなかったため320[m]となった. 一時停止できなかった要因としては自己位置推定の破綻が挙げられる. ORNE-box2では本走行前日にシングルボードコンピュータであるJetson AGX Xavierが起動しなくなるトラブルが起き, 急遽GIGABYTE GB-BXi7H-4500に交換を行っていた. これにより計算処理が間に合わなかったおそれがある. ORNE-box2においては記録走行時に完走している. 

\section{おわりに}
本稿では,千葉工業大学未来ロボティクス学科ORNEチームで開発しているロボットの概要とシステムの構成に関して述べた.また,つくばチャレンジ 2022 に向けた取り組みについて紹介した.

% \newpage

% 参考文献
% \small
\footnotesize
\begin{thebibliography}{99}

\bibitem{si-box}
井口 颯人, 石江 義規, 樋高 聖人, 上田 隆一, 林原 靖男: ``屋外自律移動ロボットプラットフォーム ORNE-boxの開発'', 3H2-03，SI2021(2021)

\bibitem{navigation}
ros-planning, navigation リポジトリ\\
\url{https://github.com/ros-planning/navigation}\\
(最終閲覧日: \today)

\bibitem{rdc}
Robot Design and Control Lab, openrdc orne\_navigation リポジトリ\\
\url{https://github.com/open-rdc/orne_navigation}\\
(最終閲覧日: \today)

\bibitem{end-to-end}
岡田 眞也, 清岡 優祐, 春山 健太, 上田 隆一, 林原 靖男: ``視覚と行動のend-to-end学習により経路追従行動をオンラインで模倣する手法の提案-“経路追従行動の修正のためにデータセットを動的に追加する手法の検討'', \textit{計測自動制御学会 SI 部門講演会 SICE-SI2021 予稿集}, pp.1066-1070, 2021.

\bibitem{emcl2-git}
ryuichiueda, emcl2 リポジトリ\\
\url{https://github.com/ryuichiueda/emcl2}\\
(最終閲覧日: \today)

\bibitem{Deeplab}
Liang-Chieh Chen , George Papandreou, Senior Member, IEEE, Iasonas Kokkinos, Member, IEEE,
Kevin Murphy, and Alan L. Yuille, Fellow, IEEE: ``DeepLab: Semantic Image Segmentation with
Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs'', \textit{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}, VOL.40, NO.4, APRIL 2018.

\bibitem{cityscapes}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Scharwachter, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele,: ``The cityscapes dataset'' in CVPR Workshop on The Future of Datasets in Vision, 2015.

\bibitem{morphological}
OpenCV公式ドキュメント モルフォロジー変換\\
\url{https://docs.opencv.org/4.x/d9/d61/tutorial_py_morphological_ops.html}
(最終閲覧日: \today)

\bibitem{meiji-thesis}
安達 美穂, 小島 一也, 石田 大貴, 松谷 幸知, 渡辺 拓斗, 小林 真吾, 横田 来夢, 坂田 唱悟, 小松崎 迅斗, 捨田利 沙羅, 宮野 龍一, 宮本 龍介: ``単眼カメラを用いた意味論的領域分割に基づくビジュアルナビゲーション'', \textit{つくばチャレンジ2019 参加レポート集}, pp105-110, 2019.

\bibitem{DeeplabV3}
deeplabv3\_plus\_pytorch\_ros リポジトリ\\
\url{https://github.com/Tsumoridesu/deeplabv3_plus_pytorch_ros/tree/add_cmd_vel}\\
(最終閲覧日: \today)

\bibitem{tsudanuma}
電子地形図3125(国土地理院)を加工して作成

\bibitem{emcl-thesis}
上田 隆一, 新井 民夫, 浅沼 和範, 梅田 和昇, 大隅 久: ``パーティクルフィルタを利用した自己位置推定に生じる致命的な推定誤りからの回復法'', \textit{日本ロボット学会誌23巻4号}, 2005.

\bibitem{box}
orne-box Github リポジトリ\\
\url{https://github.com/open-rdc/orne-box}\\
(最終閲覧日: \today)

\bibitem{mcl-3dl}
at-wat, mcl-3dl\\
\url{https://github.com/at-wat/mcl_3dl}\\
(最終閲覧日: \today)

\end{thebibliography}
\normalsize

\end{document}
