\documentclass[uplatex, twocolumn, 9pt]{jsproceedings}
\RequirePackage[l2tabu, orthodox]{nag}  % 古いコマンドやパッケージを使用した場合に警告する
\usepackage[all, warning]{onlyamsmath}  % amsmath が提供しない数式環境を使用した場合に警告する
\usepackage{flushend}  % 最終ページの2カラムの左右の高さを揃える
\usepackage{caption}
\usepackage{graphicx}

% タイトル
\title{つくばチャレンジ2022における\\千葉工業大学未来ロボティクス学科チームの取り組み}

\author{○藤原 柾, 清岡 優祐, 大塚 竜征, 石 涛, 春山 健太, 井口 颯人, 樋高 聖人, 髙橋 祐樹, 白須 和暉\newline 上田 隆一, 林原 靖男 （千葉工大）}

\etitle{Development Activity of Advanced Robotics Department Team of Chiba Institute of Technology at Tsukuba Challenge 2022}

\eauthor{Masaki FUJIWARA, Yusuke KIYOOKA, Tou SEKI, Kenta HARUYAMA, Hayato IGUCHI, Masato HIDAKA, Yuuki TAKAHASHI, Kazuki SHIRASU, Ryuichi UEDA and Yasuo HAYASHIBARA (CIT)}

\affiliation{千葉工業大学未来ロボティクス学科チームα, box, box2}

\abstract{
  In this paper, we introduce the activities of the Future Robotics Department, Advanced Engineering Division, Chiba Institute of Technology team in the Tsukuba Challenge 2022. Our team is working on the development of an outdoor autonomous mobile robot, and we are currently tackling several challenges. For example, the development of a new robot, driving using semantic segmentation, and recovery of kidnapped state by combining expansion resetting and GNSS resetting. We will report the results of these efforts.
}

% \keywords{\LaTeXe, Class File, Proceedings}

\begin{document}
\maketitle

% \authorreftext{1}{筑波大学 大学院 システム情報工学研究科}
% \eauthorreftext{1}{Graduate School of Systems and Information Engineering,\\ University of Tsukuba}

% 本文
\section{はじめに}
本チームは, 屋外で安定して自律移動するロボットを目指し, その研究および開発の一環としてつくばチャレンジに参加している. 開発したシステムは2次元地図と2D-LiDARを用いた自己位置推定により, つくばチャレンジ2016, 2017においてマイルストーン3「横断歩道区間を含まない課題コース(2037m)」を達成した. しかし, つくばチャレンジ2018から, ひらけた公園がコースに含まれるようになった. そのため, 計測距離が30m程度の2D-LiDARでは検出可能な物体が比較的少ないことから, 従来のシステムでは安定して自己位置推定を行うことが困難になった. 2018年度から計測距離が100mの3D-LiDARを採用したが, 水平面の計測データのみを使用しており, 多くのデータを破棄していた. さらに, 公園内では場所により地面の傾斜が変化するため, レーザが必ずしも水平の距離を計測していないという問題もあった. 例えば, 斜めに土が積み上げられた場所では, レーザを照射する位置が上下に変化すると, それに合わせて距離も変化してしまう. そのため, 地図生成が容易に行えないという問題もあった. 2019年度は, これらの3次元データを有効に活用するため, 2次元地図ではなく, 3次元地図を生成しMCLによる3次元自己位置推定を行い, 2次元自己位置推定と比べ, 自己位置が安定していることを確認した. しかし, 公園のような開けた場所では測域センサで検出できる物体が少なく, 測域センサ, オドメトリを用いる方法では自己位置が定まらないことがある. 本稿では, このような課題を解決するためにつくばチャレンジ2022に向けて取り組んだ開発に関して紹介する. 

\section{ロボットの概要}
つくばチャレンジでは, 本チームが開発を続けている三台のロボットORNE-α, ORNE-box\cite{si-box}, ORNE-box2を用いる. それぞれの方針は以下の通りである. 
\begin{itemize}
  \item ORNE-α\\2つの走行の切り替えによる安定した自律走行
  \item ORNE-box, ORNE-box2\\ 
\end{itemize}

\subsection{ハードウェア}
本チームは屋外自律移動ロボットとして, ORNE-α, ORNE-box, ORNE-box2の開発を行っており, つくばチャレンジ2022にはこれらのロボットが参加した. \figref{fig:orne-series}に本チームの開発している自律移動ロボットの外観を示す. これらのロボットはi-Cart mini, i-Cart middleをベースにしている.

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[width=30mm]{fig/alpha.pdf}
    \caption*{(a) ORNE-α}
  \end{minipage} 
  \hspace{0.03\columnwidth}
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[height=34mm]{fig/box.pdf}
    \caption*{(b) ORNE-box}
  \end{minipage}
  \begin{minipage}[b]{0.3\linewidth}
    \centering
    \includegraphics[height=34mm]{fig/box2.pdf}
    \caption*{(c) ORNE-box2}
  \end{minipage}
  \caption{ORNE-Series.}
  \label{fig:orne-series}%\vspace*{-2mm}
\end{figure}

\begin{table}[h]
  \centering
    \caption{Specifications of the robots}
    \scalebox{0.8}{
    \begin{tabular}{|l||c|c|c|}  \hline
       & ORNE-α & ORNE-box & ORNE-box2 \\ \hline \hline
      Depth[mm] & 690 & \multicolumn{2}{|c|}{600} \\ \hline
      Wide[mm] & 560 & \multicolumn{2}{|c|}{506.5}\\ \hline
      Height[mm] & 770 & \multicolumn{2}{|c|}{957}\\ \hline
      Wheel diameter[mm] & \multicolumn{3}{|c|}{304}\\ \hline
      Battery & LONG WP12-12 & \multicolumn{2}{|c|}{LONG WP14-12SE}\\ \hline
      Motor & \multicolumn{3}{|c|}{Oriental motor TF-M30-24-3500-G15L/R}\\ \hline
      Driving system & \multicolumn{3}{|c|}{Power wheeled steering}\\ \hline
      2D-LiDAR & URM-40LC-EW & None & UTM-30LX-EW\\
      & (HOKUYO) & & (HOKUYO)\\ \hline
      3D-LiDAR & None & R-Fans-16 & VLP-16\\
      & & (SureStar) & (Velodyne)\\ \hline
      IMU & \multicolumn{2}{|c|}{ADIS16465} & ADIS16470\\ 
      & \multicolumn{2}{|c|}{(Analog devices)} & Analog devices\\ \hline
      % GNSS receiver & None & \multicolumn{2}{|c|}{zed-f9p}\\ \hline
      Camera & CMS-V43BK & \multicolumn{2}{|c|}{None}\\ 
      & (Sanwa supply) & \multicolumn{2}{|c|}{}\\ \hline
    \end{tabular}
  }
  \end{table}

  \newpage

\subsection{ソフトウェア}
本チームでは, 従来よりROS(Robot Operating System)のnavigation stack\cite{navigation}をもとに開発されたシステムであるorne\_navigationにより自律走行させている. \figref{fig:soft-fig}に開発しているロボットのソフトウェアを含むシステム構成を示す. このシステムは, 2D-LiDARを用いたMonte Carlo Localization(MCL)により確率的に自己位置を推定し, 経路計画に基づいて自律走行している. また, 両パッケージ共にGitHubのopen-rdc\cite{rdc}でプロクラムを公開している. さらにこれをベースにORNE-boxの自律移動のパッケージとしてorne-box\cite{box}も開発している．

\begin{figure}[h]
  \centering
  \includegraphics[width=85mm]{fig/software.pdf}
  \caption{Structure of the system.}
  \label{fig:soft-fig}%\vspace*{-2mm}
\end{figure}

\section{各チームの取り組み}
本チームには, チームORNE-α, ORNE-box, ORNE-box2の3チームが存在する. 従って本章では, 各チームごとの取り組みを述べる. ただし, ORNE-box2はORNE-boxの後継機であるため, 取り組みは同じである.

\subsection{チームORNE-α}
2D-LiDARを用いた自律走行時, 自己位置推定の結果が不確かになる場合がある. この状態での走行はリタイアの要因の一つになる可能性がある. そこで, チームORNE-αは, 2D-LiDARを用いた自律走行と機械学習を用いた自律走行の切り替えによる安定した走行を目的としている. 昨年度は, orne\_navigationによる自律走行時, 自己位置推定の尤度が低下した場合に, カメラ画像を入力としたend-to-end学習器\footnote[1]{入力から出力までを一括して学習するようなニューラルネットワーク}\cite{end-to-end}の学習出力を用いた自律走行による切り替えを行った. しかし, 意図しない箇所でカメラを用いた走行へ切り替えが起こってしまうことがあった. そのため, 本年度はそれらの問題を解決するために, 取り組んだ内容に関して以下で紹介する. 

\subsubsection{提案手法}
提案手法を\figref{fig:switching}に示す. 移動ロボットは, 2つの走行方法を持つ. この2つの走行方法は, emcl2\cite{emcl2-git}のalpha\footnote[2]{センサ更新後のパーティクルの周辺尤度}を指標として切り替える.

\begin{figure}[h]
  \centering
  \includegraphics[width=82mm]{fig/switching.pdf}
  \caption{Developed system of switching action.}
  \label{fig:switching}%\vspace*{-2mm}
\end{figure}

\newpage

\begin{itemize}
  \item alphaが高い場合\\
  ①2D-LiDARを用いたナビゲーションによる自律走行
  \item alphaが低い場合(閾値以下が3秒継続時)\\
  ②セマンティックセグメンテーション\cite{Deeplab}（以下「セグメンテーション」と称する）を用いた自律走行\\
  (ただし, ②での自律走行に切り替わるのは折り返し地点からである)
\end{itemize}

\subsubsection{2D-LiDARを用いた自律走行時について}
自律走行時の構成を以下に示す.
\begin{itemize}
  \item 外界センサ: 2DLiDAR
  \item 自己位置推定: emcl2
  \item global planner: A*
  \item local planner: dwa\_local\_planner
  \item 地図作成: cartographer
  \begin{itemize}
    \item resolution: 0.15[m/pixel]
    \item 確認走行, 駅周辺, 公園の3つに分割したものを合成
  \end{itemize}
\end{itemize}

\subsubsection{セグメンテーションについて}
\begin{itemize}
  \item フレームワーク
  \begin{itemize}
    \item pytorch
  \end{itemize}
\end{itemize}
\begin{itemize}
  \item モデル
  \begin{itemize}
    \item DeepLabV3Plus-MobileNet
  \end{itemize}
\end{itemize}
\begin{itemize}
  \item データセット
  \begin{itemize}
    \item Cityscapes\cite{cityscapes}(5000枚)
    \item つくば公園エリア周辺画像(222枚)
  \end{itemize}
\end{itemize}

% \newpage

\subsubsection{セグメンテーションを用いたロボットの制御}
本チームは, セグメンテーションを用いて領域分割を行う. また, 領域分割した画像から走行可能領域(道)を検出し, その結果に基づいて行動生成を行う. 本章では, セグメンテーションを用いたロボットの制御について, 一連の流れを紹介する. 

\begin{enumerate}
  \item セグメンテーションの適用\\
  処理を施した例を\figref{fig:for_seg}に示す.
  \begin{figure}[h]
    \centering
    \includegraphics[width=60mm]{fig/camera_for_seg.pdf}
    \caption{The top image is the original image, the bottom image is the processed image using semantic segmentation.}
    \label{fig:for_seg}%\vspace*{-2mm}
  \end{figure}
  \newpage
  \item 走行可能領域の検出\\
  カメラ画像は前述のモデルを通して, \figref{fig:seg_runarea}の左画像を獲得する. 走行可能領域を抽出し, オープニング処理でノイズを除去する. 結果は\figref{fig:seg_runarea}の右画像で示す.
  \begin{figure}[h]
    \centering
    \includegraphics[width=80mm]{fig/seg_runarea.pdf}
    \caption{The left image uses semantic segmentation, and the right image shows the extracted driving area as a result.}
    \label{fig:seg_runarea}%\vspace*{-2mm}
  \end{figure}
  % \newpage
  \item 行動生成\\
  2)の処理を施した画像を用いて, \figref{fig:seg}の6つの対応する領域に応じた行動を生成する. この行動生成に関しては\cite{meiji-thesis}を参考に実装した. 例えば, \figref{fig:seg}のTurn leftの範囲に障害物が多く存在する場合, 左に曲がる方向の角速度を出力する. 
  \begin{figure}[h]
    \centering
    \includegraphics[width=60mm]{fig/seg.pdf}
    \caption{Types of behavior using semantic segmentation.}
    \label{fig:seg}%\vspace*{-2mm}
  \end{figure}
\end{enumerate}
使用したPCのスペックを\tableref{table:pc}に示す. また, GitHubのdeeplabv3\_plus\_pytorch\_ros\cite{DeeplabV3}でプログラムとデータセットを公開している.

\begin{table}[h]
  \centering
  \caption{Specification of PC}
  \label{table:pc}
  \begin{tabular}{cc}
  \toprule%[0.08em]  % デフォルト 0.08em（\heavyrulewidth）
  \textbf{CPU} & Core i7-9750H(Intel)\\
  % \midrule%[0.05em]  % デフォルト 0.05em（\lightrulewidth）
  \textbf{RAM} & 16GB\\
  \textbf{GPU} & RTX 2070 Max-Q\\
  \bottomrule%[0.08em]  % デフォルト 0.08em（\heavyrulewidth）
  \end{tabular}
% \vspace*{-2mm}
\end{table}

% \newpage
\subsubsection{本走行の結果}
今年度の本走行の記録は847.9[m]で, 駅構内の手前でリタイアとなった. これは歩行者などによりランドマークが隠されたことで, 自己位置推定の結果が不確かになり, 真の姿勢の周囲にパーティクルが存在しない状態である誘拐状態\cite{emcl-thesis}になったことが要因の一つだと考えられる. また, 折り返し地点まで到達することができなかったため, セグメンテーションを用いた自律走行に切り替わることは確認できなかった. 誘拐状態の対策としては, 自己位置推定に用いるセンサを追加することを検討する. 2つの走行方法の切り替えに関しては, alphaが低下した場合に正常に切り替わるかを検証するために千葉工業大学 津田沼キャンパス敷地内において追実験を行う. 

\subsubsection{追実験}
実験の様子を\figref{fig:switch-mode}に示す. (a), (b), (c)の順でパーティクルが拡散している際にはセグメンテーションでの走行に切り替わっていることが分かる. これらから, 2つの走行方法の切り替えが正常に行われていることが確認できた. 

\begin{figure}[h]
  \centering
  \begin{minipage}[b]{1\linewidth}
    \centering
    \includegraphics[width=70mm]{fig/switch-mode1.pdf}
    \caption*{(a) パーティクル拡散前}
  \end{minipage} 
  \hspace{0.03\columnwidth}
  \begin{minipage}[b]{1\linewidth}
    \centering
    \includegraphics[width=70mm]{fig/switch-mode2.pdf}
    \caption*{(b) パーティクル拡散}
  \end{minipage}
  \begin{minipage}[b]{1\linewidth}
    \centering
    \includegraphics[width=70mm]{fig/switch-mode3.pdf}
    \caption*{(c) パーティクル収束後}
  \end{minipage}
  \caption{Experiments on the Tsudanuma campus grounds.}
  \label{fig:switch-mode}%\vspace*{-2mm}
\end{figure}

% \newpage

\subsection{チームORNE-box, ORNE-box2}

\section{おわりに}
本稿では,千葉工業大学未来ロボティクス学科チームで開発しているロボットの概要とシステムの構成に関して述べた.また,つくばチャレンジ 2022 に向けた取り組みについて紹介した.

% \newpage

% 参考文献
% \small
\footnotesize
\begin{thebibliography}{99}

\bibitem{si-box}
井口 颯人, 石江 義規, 樋高 聖人, 上田 隆一, 林原 靖男: ``屋外自律移動ロボットプラットフォーム ORNE-boxの開発'', 3H2-03，SI2021(2021)

\bibitem{navigation}
ros-planning, navigation リポジトリ\\
\url{https://github.com/ros-planning/navigation}\\
(最終閲覧日: \today)

\bibitem{rdc}
Robot Design and Control Lab, openrdc orne\_navigation リポジトリ\\
\url{https://github.com/open-rdc/orne_navigation}\\
(最終閲覧日: \today)

\bibitem{box}
orne-box Github リポジトリ\\
\url{https://github.com/open-rdc/orne-box}\\
(最終閲覧日: \today)

\bibitem{end-to-end}
岡田 眞也, 清岡 優祐, 春山 健太, 上田 隆一, 林原 靖男: ``視覚と行動のend-to-end学習により経路追従行動をオンラインで模倣する手法の提案-“経路追従行動の修正のためにデータセットを動的に追加する手法の検討'', \textit{計測自動制御学会 SI 部門講演会 SICE-SI2021 予稿集}, pp.1066-1070, 2021.

\bibitem{emcl2-git}
ryuichiueda, emcl2 リポジトリ
\url{https://github.com/ryuichiueda/emcl2}
(最終閲覧日: \today)

\bibitem{Deeplab}
Liang-Chieh Chen , George Papandreou, Senior Member, IEEE, Iasonas Kokkinos, Member, IEEE,
Kevin Murphy, and Alan L. Yuille, Fellow, IEEE: ``DeepLab: Semantic Image Segmentation with
Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs'', \textit{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE}, VOL.40, NO.4, APRIL 2018.

\bibitem{cityscapes}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Scharwachter, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele,: ``The cityscapes dataset'' in CVPR Workshop on The Future of Datasets in Vision, 2015.

\bibitem{meiji-thesis}
安達 美穂, 小島 一也, 石田 大貴, 松谷 幸知, 渡辺 拓斗, 小林 真吾, 横田 来夢, 坂田 唱悟, 小松崎 迅斗, 捨田利 沙羅, 宮野 龍一, 宮本 龍介: ``単眼カメラを用いた意味論的領域分割に基づくビジュアルナビゲーション'', \textit{つくばチャレンジ2019 参加レポート集}, pp105-110, 2019.

\bibitem{DeeplabV3}
deeplabv3\_plus\_pytorch\_ros レポジトリ\\
\url{https://github.com/Tsumoridesu/deeplabv3_plus_pytorch_ros/tree/add_cmd_vel}\\
(最終閲覧日: \today)

\bibitem{emcl-thesis}
上田 隆一, 新井 民夫, 浅沼 和範, 梅田 和昇, 大隅 久: ``パーティクルフィルタを利用した自己位置推定に生じる致命的な推定誤りからの回復法'', \textit{日本ロボット学会誌23巻4号}, 2005.

\end{thebibliography}
\normalsize

\end{document}
